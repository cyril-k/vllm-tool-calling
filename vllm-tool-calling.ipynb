{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named `tool_call` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launch vLLM server in terminal with the following:\n",
    "\n",
    "python3 -m vllm.entrypoints.openai.api_server --model meta-llama/Meta-Llama-3.1-70B-Instruct --dtype bfloat16 --tensor_parallel_size 2 --max_model_len 4096 --max_num_seq 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "MODEL_NAME = \"meta-llama/Meta-Llama-3.1-70B-Instruct\"\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:8000/v1/\",\n",
    "    api_key=\"-\",\n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"What is the capital of France?\"}],\n",
    "    temperature=0.6,\n",
    "    max_tokens=512,\n",
    "    top_p=0.9\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Named tool calling example from vLLM unit tests: https://github.com/vllm-project/vllm/blob/9d104b5beb7bbb51c64b680e007f39169489ea86/tests/entrypoints/openai/test_chat.py#L637"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import jsonschema\n",
    "\n",
    "sample_json_schema = {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"name\": {\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"age\": {\n",
    "                \"type\": \"integer\"\n",
    "            },\n",
    "            \"skills\": {\n",
    "                \"type\": \"array\",\n",
    "                \"items\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"maxLength\": 10\n",
    "                },\n",
    "                \"minItems\": 3\n",
    "            },\n",
    "            \"work_history\": {\n",
    "                \"type\": \"array\",\n",
    "                \"items\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"company\": {\n",
    "                            \"type\": \"string\"\n",
    "                        },\n",
    "                        \"duration\": {\n",
    "                            \"type\": \"number\"\n",
    "                        },\n",
    "                        \"position\": {\n",
    "                            \"type\": \"string\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"company\", \"position\"]\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"name\", \"age\", \"skills\", \"work_history\"]\n",
    "    }\n",
    "\n",
    "\n",
    "messages = [{\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"you are a helpful assistant\"\n",
    "    }, {\n",
    "        \"role\":\n",
    "        \"user\",\n",
    "        \"content\":\n",
    "        f\"Give an example JSON for an employee profile that \"\n",
    "        f\"fits this schema: {sample_json_schema}\"\n",
    "    }]\n",
    "\n",
    "# non-streaming\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    messages=messages,\n",
    "    max_tokens=1000,\n",
    "    tools=[{\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"dummy_function_name\",\n",
    "            \"description\": \"This is a dummy function\",\n",
    "            \"parameters\": sample_json_schema\n",
    "        }\n",
    "    }],\n",
    "    tool_choice={\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"dummy_function_name\"\n",
    "        }\n",
    "    })\n",
    "message = chat_completion.choices[0].message\n",
    "assert len(message.content) == 0\n",
    "json_string = message.tool_calls[0].function.arguments\n",
    "json1 = json.loads(json_string)\n",
    "jsonschema.validate(instance=json1, schema=sample_json_schema)\n",
    "\n",
    "messages.append({\"role\": \"assistant\", \"content\": json_string})\n",
    "messages.append({\n",
    "    \"role\":\n",
    "    \"user\",\n",
    "    \"content\":\n",
    "    \"Give me another one with a different name and age\"\n",
    "})\n",
    "\n",
    "# streaming\n",
    "\n",
    "stream = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    messages=messages,\n",
    "    max_tokens=1000,\n",
    "    tools=[{\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"dummy_function_name\",\n",
    "            \"description\": \"This is a dummy function\",\n",
    "            \"parameters\": sample_json_schema\n",
    "        }\n",
    "    }],\n",
    "    tool_choice={\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"dummy_function_name\"\n",
    "        }\n",
    "    },\n",
    "    stream=True)\n",
    "\n",
    "output = []\n",
    "finish_reason_count = 0\n",
    "for chunk in stream:\n",
    "    delta = chunk.choices[0].delta\n",
    "    if delta.role:\n",
    "        assert delta.role == \"assistant\"\n",
    "    assert delta.content is None or len(delta.content) == 0\n",
    "    if delta.tool_calls:\n",
    "        output.append(delta.tool_calls[0].function.arguments)\n",
    "    if chunk.choices[0].finish_reason is not None:\n",
    "        finish_reason_count += 1\n",
    "# finish reason should only return in last block\n",
    "assert finish_reason_count == 1\n",
    "json2 = json.loads(\"\".join(output))\n",
    "jsonschema.validate(instance=json2, schema=sample_json_schema)\n",
    "assert json1[\"name\"] != json2[\"name\"]\n",
    "assert json1[\"age\"] != json2[\"age\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If asserts pass then named tool calling works correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`tool_call` #1 with chat completion: \n",
      "{'age': 30, 'name': 'John Doe', 'skills': ['Python', 'Java', 'C++'], 'work_history': [{'company': 'ABC Corp', 'duration': 3, 'position': 'Software Engineer'}]}\n"
     ]
    }
   ],
   "source": [
    "print(f\"`tool_call` #1 with chat completion: \\n{json1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`tool_call` #2 with streaming chat completion: \n",
      "{'age': 35, 'name': 'Jane Smith', 'skills': ['JavaScript', 'HTML', 'CSS'], 'work_history': [{'company': 'XYZ Inc', 'duration': 5, 'position': 'Web Developer'}]}\n"
     ]
    }
   ],
   "source": [
    "print(f\"`tool_call` #2 with streaming chat completion: \\n{json2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto `tool_call` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples from: https://github.com/vllm-project/vllm/blob/main/examples/openai_chat_completion_client_with_tools.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## with Mistral model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launch vLLM server in terminal with the following:\n",
    "\n",
    "python3 -m vllm.entrypoints.openai.api_server \\\n",
    "    --model mistralai/Mistral-7B-Instruct-v0.3 \\\n",
    "    --chat-template examples/tool_chat_template_mistral.jinja \\\n",
    "    --enable-auto-tool-choice --tool-call-parser mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat completion results:\n",
      "ChatCompletion(id='chat-fefb016290d944aba28e6c03dd6c8c70', choices=[Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='chatcmpl-tool-7e758fdfc45e4d648cf50744af4cb0f0', function=Function(arguments='{\"city\": \"Dallas\", \"state\": \"TX\", \"unit\": \"fahrenheit\"}', name='get_current_weather'), type='function')]), stop_reason=None)], created=1726661479, model='mistralai/Mistral-7B-Instruct-v0.3', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=40, prompt_tokens=216, total_tokens=256, completion_tokens_details=None), prompt_logprobs=None)\n",
      "\n",
      "\n",
      "\n",
      "ChoiceDelta(content='', function_call=None, refusal=None, role='assistant', tool_calls=None)\n",
      "ChoiceDeltaToolCall(index=0, id='chatcmpl-tool-d9e99c0046e64d418e05bbb4f82e0801', function=ChoiceDeltaToolCallFunction(arguments=None, name='get_current_weather'), type='function')\n",
      "ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='{\"city\": \"', name=None), type=None)\n",
      "ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='D', name=None), type=None)\n",
      "ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='allas', name=None), type=None)\n",
      "ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='', name=None), type=None)\n",
      "ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='', name=None), type=None)\n",
      "ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='', name=None), type=None)\n",
      "ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='', name=None), type=None)\n",
      "ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='\", \"state\": \"', name=None), type=None)\n",
      "ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='TX', name=None), type=None)\n",
      "ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='', name=None), type=None)\n",
      "ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='', name=None), type=None)\n",
      "ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='', name=None), type=None)\n",
      "ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='', name=None), type=None)\n",
      "ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='\", \"unit\": \"', name=None), type=None)\n",
      "ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='f', name=None), type=None)\n",
      "ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='ahren', name=None), type=None)\n",
      "ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='heit', name=None), type=None)\n",
      "ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='', name=None), type=None)\n",
      "ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='', name=None), type=None)\n",
      "ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='', name=None), type=None)\n",
      "ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='\"}', name=None), type=None)\n",
      "streamed tool call id: chatcmpl-tool-d9e99c0046e64d418e05bbb4f82e0801 \n",
      "streamed tool call name: get_current_weather\n",
      "streamed tool call arguments: {\"city\": \"Dallas\", \"state\": \"TX\", \"unit\": \"fahrenheit\"}\n",
      "\n",
      "\n",
      "\n",
      "The weather in Dallas, Texas is 85 degrees fahrenheit. It is partly cloudly, with highs in the 90's.\n",
      "\n",
      "\n",
      "\n",
      "ChatCompletion(id='chat-c12ed8dee04946f4baf89ee604b6f383', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\" The weather in Dallas, Texas is 85 degrees fahrenheit. It is partly cloudly, with highs in the 90's. Enjoy your day!\", refusal=None, role='assistant', function_call=None, tool_calls=[]), stop_reason=None)], created=1726661479, model='mistralai/Mistral-7B-Instruct-v0.3', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=37, prompt_tokens=318, total_tokens=355, completion_tokens_details=None), prompt_logprobs=None)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "# Modify OpenAI's API key and API base to use vLLM's API server.\n",
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = \"http://localhost:8000/v1\"\n",
    "\n",
    "client = OpenAI(\n",
    "    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")\n",
    "\n",
    "models = client.models.list()\n",
    "model = models.data[0].id\n",
    "\n",
    "tools = [{\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"get_current_weather\",\n",
    "        \"description\": \"Get the current weather in a given location\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"city\": {\n",
    "                    \"type\":\n",
    "                    \"string\",\n",
    "                    \"description\":\n",
    "                    \"The city to find the weather for, e.g. 'San Francisco'\"\n",
    "                },\n",
    "                \"state\": {\n",
    "                    \"type\":\n",
    "                    \"string\",\n",
    "                    \"description\":\n",
    "                    \"the two-letter abbreviation for the state that the city is\"\n",
    "                    \" in, e.g. 'CA' which would mean 'California'\"\n",
    "                },\n",
    "                \"unit\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The unit to fetch the temperature in\",\n",
    "                    \"enum\": [\"celsius\", \"fahrenheit\"]\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"city\", \"state\", \"unit\"]\n",
    "        }\n",
    "    }\n",
    "}]\n",
    "\n",
    "messages = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"Hi! How are you doing today?\"\n",
    "}, {\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": \"I'm doing well! How can I help you?\"\n",
    "}, {\n",
    "    \"role\":\n",
    "    \"user\",\n",
    "    \"content\":\n",
    "    \"Can you tell me what the temperate will be in Dallas, in fahrenheit?\"\n",
    "}]\n",
    "\n",
    "chat_completion = client.chat.completions.create(messages=messages,\n",
    "                                                 model=model,\n",
    "                                                 tools=tools)\n",
    "\n",
    "print(\"Chat completion results:\")\n",
    "print(chat_completion)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "tool_calls_stream = client.chat.completions.create(messages=messages,\n",
    "                                                   model=model,\n",
    "                                                   tools=tools,\n",
    "                                                   stream=True)\n",
    "\n",
    "chunks = []\n",
    "for chunk in tool_calls_stream:\n",
    "    chunks.append(chunk)\n",
    "    if chunk.choices[0].delta.tool_calls:\n",
    "        print(chunk.choices[0].delta.tool_calls[0])\n",
    "    else:\n",
    "        print(chunk.choices[0].delta)\n",
    "\n",
    "arguments = []\n",
    "tool_call_idx = -1\n",
    "for chunk in chunks:\n",
    "\n",
    "    if chunk.choices[0].delta.tool_calls:\n",
    "        tool_call = chunk.choices[0].delta.tool_calls[0]\n",
    "\n",
    "        if tool_call.index != tool_call_idx:\n",
    "            if tool_call_idx >= 0:\n",
    "                print(\n",
    "                    f\"streamed tool call arguments: {arguments[tool_call_idx]}\"\n",
    "                )\n",
    "            tool_call_idx = chunk.choices[0].delta.tool_calls[0].index\n",
    "            arguments.append(\"\")\n",
    "        if tool_call.id:\n",
    "            print(f\"streamed tool call id: {tool_call.id} \")\n",
    "\n",
    "        if tool_call.function:\n",
    "            if tool_call.function.name:\n",
    "                print(f\"streamed tool call name: {tool_call.function.name}\")\n",
    "\n",
    "            if tool_call.function.arguments:\n",
    "                arguments[tool_call_idx] += tool_call.function.arguments\n",
    "\n",
    "if len(arguments):\n",
    "    print(f\"streamed tool call arguments: {arguments[-1]}\")\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "messages.append({\n",
    "    \"role\": \"assistant\",\n",
    "    \"tool_calls\": chat_completion.choices[0].message.tool_calls\n",
    "})\n",
    "\n",
    "\n",
    "# Now, simulate a tool call\n",
    "def get_current_weather(city: str, state: str, unit: 'str'):\n",
    "    return (\"The weather in Dallas, Texas is 85 degrees fahrenheit. It is \"\n",
    "            \"partly cloudly, with highs in the 90's.\")\n",
    "\n",
    "\n",
    "available_tools = {\"get_current_weather\": get_current_weather}\n",
    "\n",
    "completion_tool_calls = chat_completion.choices[0].message.tool_calls\n",
    "for call in completion_tool_calls:\n",
    "    tool_to_call = available_tools[call.function.name]\n",
    "    args = json.loads(call.function.arguments)\n",
    "    result = tool_to_call(**args)\n",
    "    print(result)\n",
    "    messages.append({\n",
    "        \"role\": \"tool\",\n",
    "        \"content\": result,\n",
    "        \"tool_call_id\": call.id,\n",
    "        \"name\": call.function.name\n",
    "    })\n",
    "\n",
    "chat_completion_2 = client.chat.completions.create(messages=messages,\n",
    "                                                   model=model,\n",
    "                                                   tools=tools,\n",
    "                                                   stream=False)\n",
    "print(\"\\n\\n\")\n",
    "print(chat_completion_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## with NousResearch model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launch vLLM server in terminal with the following:\n",
    "\n",
    "python3 -m vllm.entrypoints.openai.api_server \\\n",
    "    --model NousResearch/Hermes-2-Pro-Llama-3-8B \\\n",
    "    --chat-template examples/tool_chat_template_hermes.jinja \\\n",
    "    --enable-auto-tool-choice --tool-call-parser hermes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat completion results:\n",
      "ChatCompletion(id='chat-e9d782cdc1a54487aebb1c60eb73da7b', choices=[Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='chatcmpl-tool-625c8eafcab74ac78ed5fc2c4a16cd7c', function=Function(arguments='{\"city\": \"Dallas\", \"state\": \"TX\", \"unit\": \"fahrenheit\"}', name='get_current_weather'), type='function')]), stop_reason=None)], created=1726661979, model='NousResearch/Hermes-2-Pro-Llama-3-8B', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=34, prompt_tokens=464, total_tokens=498, completion_tokens_details=None), prompt_logprobs=None)\n",
      "\n",
      "\n",
      "\n",
      "ChoiceDelta(content='', function_call=None, refusal=None, role='assistant', tool_calls=None)\n",
      "ChoiceDeltaToolCall(index=0, id='chatcmpl-tool-499222d348ce4451aba006b283e5ea6e', function=ChoiceDeltaToolCallFunction(arguments=None, name='get_current_weather'), type='function')\n",
      "ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='{\"city\": \"', name=None), type=None)\n",
      "ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='Dallas', name=None), type=None)\n",
      "ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='', name=None), type=None)\n",
      "ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='', name=None), type=None)\n",
      "ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='', name=None), type=None)\n",
      "ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='', name=None), type=None)\n",
      "ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='\", \"state\": \"', name=None), type=None)\n",
      "ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='TX', name=None), type=None)\n",
      "ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='', name=None), type=None)\n",
      "ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='', name=None), type=None)\n",
      "ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='', name=None), type=None)\n",
      "ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='', name=None), type=None)\n",
      "ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='\", \"unit\": \"', name=None), type=None)\n",
      "ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='f', name=None), type=None)\n",
      "ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='ahrenheit', name=None), type=None)\n",
      "ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='', name=None), type=None)\n",
      "ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='\"}', name=None), type=None)\n",
      "ChoiceDelta(content='', function_call=None, refusal=None, role=None, tool_calls=None)\n",
      "streamed tool call id: chatcmpl-tool-499222d348ce4451aba006b283e5ea6e \n",
      "streamed tool call name: get_current_weather\n",
      "streamed tool call arguments: {\"city\": \"Dallas\", \"state\": \"TX\", \"unit\": \"fahrenheit\"}\n",
      "\n",
      "\n",
      "\n",
      "The weather in Dallas, Texas is 85 degrees fahrenheit. It is partly cloudly, with highs in the 90's.\n",
      "\n",
      "\n",
      "\n",
      "ChatCompletion(id='chat-82658a36a68f472e832c78075a7124cc', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The current temperature in Dallas, Texas is 85 degrees Fahrenheit. It is partly cloudy, with highs expected in the 90s.', refusal=None, role='assistant', function_call=None, tool_calls=[]), stop_reason=None)], created=1726661980, model='NousResearch/Hermes-2-Pro-Llama-3-8B', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=28, prompt_tokens=536, total_tokens=564, completion_tokens_details=None), prompt_logprobs=None)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "# Modify OpenAI's API key and API base to use vLLM's API server.\n",
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = \"http://localhost:8000/v1\"\n",
    "\n",
    "client = OpenAI(\n",
    "    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")\n",
    "\n",
    "models = client.models.list()\n",
    "model = models.data[0].id\n",
    "\n",
    "tools = [{\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"get_current_weather\",\n",
    "        \"description\": \"Get the current weather in a given location\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"city\": {\n",
    "                    \"type\":\n",
    "                    \"string\",\n",
    "                    \"description\":\n",
    "                    \"The city to find the weather for, e.g. 'San Francisco'\"\n",
    "                },\n",
    "                \"state\": {\n",
    "                    \"type\":\n",
    "                    \"string\",\n",
    "                    \"description\":\n",
    "                    \"the two-letter abbreviation for the state that the city is\"\n",
    "                    \" in, e.g. 'CA' which would mean 'California'\"\n",
    "                },\n",
    "                \"unit\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The unit to fetch the temperature in\",\n",
    "                    \"enum\": [\"celsius\", \"fahrenheit\"]\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"city\", \"state\", \"unit\"]\n",
    "        }\n",
    "    }\n",
    "}]\n",
    "\n",
    "messages = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"Hi! How are you doing today?\"\n",
    "}, {\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": \"I'm doing well! How can I help you?\"\n",
    "}, {\n",
    "    \"role\":\n",
    "    \"user\",\n",
    "    \"content\":\n",
    "    \"Can you tell me what the temperate will be in Dallas, in fahrenheit?\"\n",
    "}]\n",
    "\n",
    "chat_completion = client.chat.completions.create(messages=messages,\n",
    "                                                 model=model,\n",
    "                                                 tools=tools)\n",
    "\n",
    "print(\"Chat completion results:\")\n",
    "print(chat_completion)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "tool_calls_stream = client.chat.completions.create(messages=messages,\n",
    "                                                   model=model,\n",
    "                                                   tools=tools,\n",
    "                                                   stream=True)\n",
    "\n",
    "chunks = []\n",
    "for chunk in tool_calls_stream:\n",
    "    chunks.append(chunk)\n",
    "    if chunk.choices[0].delta.tool_calls:\n",
    "        print(chunk.choices[0].delta.tool_calls[0])\n",
    "    else:\n",
    "        print(chunk.choices[0].delta)\n",
    "\n",
    "arguments = []\n",
    "tool_call_idx = -1\n",
    "for chunk in chunks:\n",
    "\n",
    "    if chunk.choices[0].delta.tool_calls:\n",
    "        tool_call = chunk.choices[0].delta.tool_calls[0]\n",
    "\n",
    "        if tool_call.index != tool_call_idx:\n",
    "            if tool_call_idx >= 0:\n",
    "                print(\n",
    "                    f\"streamed tool call arguments: {arguments[tool_call_idx]}\"\n",
    "                )\n",
    "            tool_call_idx = chunk.choices[0].delta.tool_calls[0].index\n",
    "            arguments.append(\"\")\n",
    "        if tool_call.id:\n",
    "            print(f\"streamed tool call id: {tool_call.id} \")\n",
    "\n",
    "        if tool_call.function:\n",
    "            if tool_call.function.name:\n",
    "                print(f\"streamed tool call name: {tool_call.function.name}\")\n",
    "\n",
    "            if tool_call.function.arguments:\n",
    "                arguments[tool_call_idx] += tool_call.function.arguments\n",
    "\n",
    "if len(arguments):\n",
    "    print(f\"streamed tool call arguments: {arguments[-1]}\")\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "messages.append({\n",
    "    \"role\": \"assistant\",\n",
    "    \"tool_calls\": chat_completion.choices[0].message.tool_calls\n",
    "})\n",
    "\n",
    "\n",
    "# Now, simulate a tool call\n",
    "def get_current_weather(city: str, state: str, unit: 'str'):\n",
    "    return (\"The weather in Dallas, Texas is 85 degrees fahrenheit. It is \"\n",
    "            \"partly cloudly, with highs in the 90's.\")\n",
    "\n",
    "\n",
    "available_tools = {\"get_current_weather\": get_current_weather}\n",
    "\n",
    "completion_tool_calls = chat_completion.choices[0].message.tool_calls\n",
    "for call in completion_tool_calls:\n",
    "    tool_to_call = available_tools[call.function.name]\n",
    "    args = json.loads(call.function.arguments)\n",
    "    result = tool_to_call(**args)\n",
    "    print(result)\n",
    "    messages.append({\n",
    "        \"role\": \"tool\",\n",
    "        \"content\": result,\n",
    "        \"tool_call_id\": call.id,\n",
    "        \"name\": call.function.name\n",
    "    })\n",
    "\n",
    "chat_completion_2 = client.chat.completions.create(messages=messages,\n",
    "                                                   model=model,\n",
    "                                                   tools=tools,\n",
    "                                                   stream=False)\n",
    "print(\"\\n\\n\")\n",
    "print(chat_completion_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm-tool-call-A5D1NQoR-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
